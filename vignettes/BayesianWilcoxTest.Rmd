---
title: "A Bayesian Alternative to the Wilcoxon Rank Test"
author: "Joachim Reinhardt"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: Rbibliography.bib
vignette: >
  %\VignetteIndexEntry{A Bayesian Alternative to the Wilcoxon Rank Test}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The **bayes.wilcox.test** R-package implements a Bayesian Alternative to the classical
Wilcoxon Rank test and is intended as an addition to the **BayesianFirstAid** 
package [@baath2014bayesian]. As in the original Wilcoxon test, the data are 
substituted by their ranks. Following @gelman2014bayesian, p. 97, the ranks
are then transformed to the quantiles of a standard Gaussian
distribution. This allows modeling the transformed data as a Gaussian
distribution via Bayesian methods with uninformative priors. The resulting 
posterior distribution can be interpreted similarly to the classical hypothesis
test, but provides more detailed information and has a more straightforward
interpretation.  
The package implements a paired and independent sample test.  

## Introduction

In a frequentist framework, hypothesis testing is performed in a way that is
distinct from other forms of inference, such as parameter estimation. In a 
frequentist hypothesis test, the compability of the observed data with a given
hypothesis on model parameters is assessed, while parameter estimation - for 
example by maximum likelihood methods - is concerned with finding the parameter
estimate most compatible with the given data. In probability notation, 
frequentist hypothesis tests evaluate $P(y|\theta_0)$, known as the p-value -
i.e. the probability of obtaining the observed data $y$ under the assumption 
that the model under the null hypothesis, with parameters $\theta_0$, is the 
true process by which the data are generated. Frequentist maximum likelihood 
parameter estimation operates by finding the value of $\theta$ for 
which the likelihood ${\cal L}(\theta|y)$ given the observed data is maximal, 
which corresponds to the parameter with a p-value of $1$, or $P(y|\theta) = 1$.
  
  
From a Bayesian perspective, parameter estimates and hypothesis tests on 
parameters both involve the same estimation procedure. The outcome of 
Bayesian model estimation is a full probability distribution of the parameters
in question, the posterior distribution $P(\theta|y)$. This posterior 
distribution can directly be employed to test hypotheses on the parameters at 
hand.  

One way of doing so are Bayes factors, which compare the posterior distributions
of two models, usually under a null and an alternative hypothesis. The ratio of
both posterior distributions indicates which model can be considered likely 
given the observed data. A major difference to frequentist hypothesis tests is
that evidence both for and against the null hypothesis is considered; thus, the
equivalent of "accepting" the null hypothesis is possible. The drawback of 
using Bayes Factors for hypothesis testing is that they can only be usefully 
employed for clearly discrete alternatives of parameter values, as noted by
@gelman2014bayesian, p. 182. For continuous models with infinitely many 
parameter values between alternative hypotheses, the use of Bayes factors is not
helpful.  

### The BEST Approach

Another way of Bayesian hypothesis testing is presented by 
@kruschke2013bayesian, who describes a Bayesian alternative to the
frequentist t-test. This approach was extended to different types of hypothesis
tests in the **BayesianFirstAid** R-package [@baath2014bayesian], and this the 
approach is followed for the **bayes.wilcox.test** package. Kruschke's BEST
("Bayesian Estimation Supersedes the t Test") procedure for the two-sample test
models the data as Student's t distributions with uninformative priors. The 
resulting posterior distributions for the $\mu$ parameter of both groups give 
the probabilies of the means exceeding or falling below any given value, and 
the probability of their differences ...  
Advantages of this testing procedure, etc.  

### The Frequentist Wilcoxon

Hypothesis tests such as the frequentist t-Test and BEST are based on the 
assumption that the data follow a Gaussian (for the frequentist test) or a 
Student's t distribution (for BEST). In addition, the scale of measurement of 
the data needs to be an interval scale, which excludes data on an 
ordinal scale such as grades or ratings. For cases in which these assumptions
are violated, non-parametric alternatives exist: the Wilcoxon Signed Rank test
for paired samples, due to @wilcoxon1945individual, and the independent-sample
Wilcoxon Rank Sum test, also known as the Mann-Whitney test, first proposed by
@wilcoxon1945individual and further developed by @mann1947test.


  

In the following, a similar Bayesian testing procedure for the frequentist 
Wilcoxon Rank test will be presented.



## The Bayesian Wilcoxon Rank Test

The present section describes the estimation procedure for Bayesian Alternative 
to the frequentist Wilcoxon Rank test which is implemented by the
**bayes.wilcox.test** package.

### Nonlinear transformation to Gaussian quantiles

The Bayesian Wilcoxon test follows an estimation procedure outlined in 
@gelman2014bayesian and further described in a blog post by Andrew Gelman
[@gelman2015dont] for a two-sample test. As in the frequentist Wilcoxon 
procedure, the data are substituted by their ranks. Subsequently, the ranks
are transformed to quantiles of a standard Gaussian distribution by applying a
Gaussian quantile function to the sequence  

$\frac{1}{2n}, \frac{3}{2n}, ..., \frac{2i-1}{2n}, ..., \frac{2n - 1}{2n}$  

which results in a discrete distribution which approaches a standard Gaussian 
distribution as $n$ increases. A similar approach has been proposed 
by @van1953neuer, who suggested using a sequence of equidistant points instead 
of the sequence (). The differences between the resulting transformed ranks for
two groups can in turn be modeled as a Gaussian distribution.  
Via Bayesian estimation, 

### Prior Distributions
(Two-sample case first)  

One-sample case: Assuming $\mu_{diff}$ to come from $N(\mu, 1)$ distribution.
Thus, a prior is needed only for $\mu$. Maximum and Minimum value of 
$\mu_{diff}$:
Occur when all values from sample $x$ are larger than the values from sample
$y$. Figure out Min. / Max. by Simulation:  

Generate $n$ nonequal values for $x$ and $y$, where $x_i > y_i$ for all $i$.
Perform rank-z-score transform and calculate the mean z-score for $x$ and $y$:  
$\bar{z}_x = \frac{1}{n} \sum^n_{i=1} z_{x_i}$, equivalently for $y$. For large
$n$, $\bar{z}_x-\bar{z}_y \to 1.5958$. Exchanging $x$ and $y$ leads to 
$\bar{z}_y - \bar{z}_x \to -1.5958$. Employing a uniformly distributed prior,
the prior for $\mu_{diff}$ is $Unif(-1.5958, 1.5958)$. (Check if there is a
more flexible $Beta$-representation)

For two sample case:  
$\mu_x$ and $\mu_y$ come from separate $N(\mu, \sigma)$ distributions -- priors
are needed for both $\mu$ and $\sigma$. From simulation, large $n$: 
$\bar{z}_x \to 0.7979$, $\bar{z}_y \to -0.7979$. Thus, priors for $\mu_x$ and
$\mu_y$ are set to $Unif(-0.7979, 0.7979)$.  
For priors on $\sigma$, the simulation should also give the lower limit for 
possible values of $\sigma$ (is this true?). Results give  
$\sigma_{x_L} = \sigma_{y_L} \to 0.6028$. The upper limit is given by 
$\sigma_H = 1$, if $x$ and $y$ are "maximally distributed" within the $N(0,1)$
parent distribution. Priors for $\sigma$ thus set to $Unif(0.6027, 1)$.  

Note [@van1953neuer]: normalized values have discrete distribution!
asymptoticallly normal distributed. But we don't know their variance (only
within range). t-Test more appropriate?


## The bayes.wilcox.test Package



## Notes

[@gelman2011bayesian, p.11]: "Even the Wilcoxon Test is based on assumptions!"

Note: Consider Mann-Whitney Test! -> test for two independent groups, also
included in wilcox.test function

Gelman (blog post): "the Wilcoxon test has a huge assumption which is that the 
distributions are identical in the two groups. This is a much more important 
assumption than normality." Cite blog post

"History of the idea": [@van1953neuer] introduced concept, but used equidistant
points instead of quantiles. [@gelman2014bayesian, p. 97] using quantiles

[@kruschke2013bayesian, p. 575]: "In the case of data from a single
group, including the case of a single group of difference scores
from repeated measures on the same subjects, a modified model
merely estimates $\mu$, $\sigma$, and $\nu$ of the group." Crucial change from
independent two paired sample test: two different $\sigma$s vs. a single one
See Kruschke's 2011 discussion of oneway / twoway ANOVA!

Consider including effect size / How to deal with mu?



## References
